
\chapter{Desarrollo}
\label{cha:desarrollo}

\begin{FraseCelebre}
  \begin{Frase}
    Cada uno de nosotros debe trabajar para su propia mejora, y al mismo tiempo compartir una responsabilidad general para toda la humanidad.
  \end{Frase}
  \begin{Fuente}
    Marie Curie
  \end{Fuente}
\end{FraseCelebre}

\section{Introducción}
\label{sec:intro-desarrollo}

En el presente \gls{tfm} se ha desarrollado una estrategia de detección de objetos abandonados para ser aplicado en sistemas de videovigilancia. Como ya se comentó al final de la sección \ref{subsec:comparativa-detectores}, se utilizará como detector de objetos y personas \gls{yolov4} y como algoritmo de seguimiento \gls{deepsort}. A continuación, se describen las diferentes secciones que componen este capítulo.

Primero se evaluará YOLOv4 sobre Darknet, framework de código abierto escrito en C y \gls{cuda}, y se analizará la precisión y velocidad que se obtienen en la detección de objetos y personas. Posteriormente se convertirá el modelo de \gls{yolov4} de Darknet a Tensorflow, framework de código abierto escrito en Python y C++ orientado al desarrollo de algoritmos inteligentes de Machine Learning. Utilizar \gls{yolov4} con Tensorflow facilitará la programación del algoritmo de detección de objetos abandonados con Python, ya que Darknet no es un framework de uso extendido y podría ser más difícil encontrar soluciones para los posibles errores. A continuación se reentrenará el modelo de la red \gls{yolov4} sobre el dataset \gls{oidv4} para observar si se obtienen mayores valores en las métricas de calidad respecto a \gls{coco}.

Una vez obtenido el modelo de \gls{yolov4} definitivo, se probará el algoritmo de seguimiento \gls{deepsort}. Solo nos interesa la detección y seguimiento de personas y objetos de interés, por lo que se filtrará la detección para que solo se identifiquen las clases que queremos.

Finalmente, se expondrá una estrategia para la detección de objetos abandonados basada en la detección de objetos y personas mediante \gls{cnn}'s y se implementará sobre el detector de objetos \gls{yolov4} junto al algoritmo de detección \gls{deepsort}. En el planteamiento del algoritmo de detección de objetos abandonados se tendrá que considerar dos posibles escenarios. El primero es que se identifique un objeto sin propietario que se encuentre estacionario durante toda la ejecución de la secuencia de vídeo. En este caso, se emitirá una señal de alarma cuando se superen los 15 segundos del objeto inmóvil. En el segundo escenario se deberá de crear una asociación entre persona y objeto. Una vez establecida la asociación se podrá evaluar cuando una persona abandona un objeto de su propiedad a una una distancia en píxeles 5 veces mayor a la distancia a la que se encontraba en el momento que se realizó la asociación.

Cabe recalcar que en este capítulo se va a exponer cada uno de los procedimientos que se han llevado a cabo para poner en funcionamiento los algoritmos de detección, seguimiento y detección de objetos abandonados. Todos los resultados obtenidos durante el desarrollo de esta parte del proyecto se pueden consultar en el capítulo \ref{cha:resultados}.

\section{Detección de personas y objetos con YOLOv4}
\label{sec:desarrollo-yolov4}



Para la utilización de \gls{yolov4} se ha instalado previamente la librería opencv-python 4.1.1.26 junto con \gls{cuda} 10.1.243 y cuDNN 7.6.5 para poder codificar de manera más optima los algoritmos utilizando una \gls{gpu} NVIDIA.

Se ha clonado el repositorio de \gls{yolov4} de GitHub \cite{yolov4-darknet-github} en el framework Darknet. Para obtener el máximo rendimiento por parte del equipo que se está utilizando se ha modificado el fichero Makefile para habilitar la utilización de la \gls{gpu} junto con \gls{cuda}, así como OpenCV para poder visualizar por pantalla los resultados las de las detecciones.

En la evaluación del algoritmo de detección de \gls{yolov4} se va a emplear una \gls{gpu} NVIDIA Tesla T4 del servicio cloud de Google Colab. Esa \gls{gpu} tiene una capacidad de computación de 7,5 por lo que será necesario descomentar una línea del fichero Makefile para indicar que se está empleando una \gls{gpu} con esas características. Tras modificar el fichero se ha ejecutado el comando make para compilar el repositorio, de tal manera que se generan los ficheros necesarios para el funcionamiento de la red neuronal de \gls{yolov4} sobre Darknet. 

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación del detector de objetos YOLOv4 en Darknet (1),captionpos=b,label={lst:evaluate-yolov4-darknet1}]
# Clonar el repositorio de GitHub de YOLOv4 darknet
git clone https://github.com/AlexeyAB/darknet

# Entrar dentro de la carpeta del repositorio
cd darknet

# Modificar las siguientes lineas del fichero makefile para habilitar la GPU y OPENCV
sed -i 's/OPENCV=0/OPENCV=1/' Makefile
sed -i 's/GPU=0/GPU=1/' Makefile
sed -i 's/CUDNN=0/CUDNN=1/' Makefile
sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile
sed -i 's/# ARCH= -gencode arch=compute_75/ARCH= -gencode arch=compute_75/' Makefile

# Compilar mediante el comando make
make
\end{lstlisting}

Otro fichero que es necesario de modificar antes de ejecutar el algoritmo de detección es el fichero yolov4.cfg que es encuentra dentro de la carpeta \texttt{./cfg}. Este fichero contiene parámetros de configuración de la \gls{cnn} de \gls{yolov4} como el learning rate, max batches, momentum o hue, necesarios para el entrenamiento de la red, las dimensiones de las imágenes que se introducen a la capa de entrada de la red o los filtros y clases que se deben de tener en cuenta tanto para el entrenamiento como para la evaluación.

Se ha modificado los parámetros de \texttt{batch} y \texttt{subdivisions} con un valor de 1, ya que se va a usar la red para testear y no para entrenarla. Estos parámetros determinan la cantidad de imágenes que son procesadas de manera paralela.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación del detector de objetos YOLOv4 en Darknet (2),captionpos=b,label={lst:evaluate-yolov4-darknet2}]
# Cambiar batch y subdivisions de yolov4.cfg para test
cd cfg
sed -i 's/batch=64/batch=1/' yolov4.cfg
sed -i 's/subdivisions=8/subdivisions=1/' yolov4.cfg
cd ..

# Descargar los pesos preentrenados de YOLOv4
wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights

# Ejecutar detector de objetos en video con YOLOv4 en Darknet
./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights ./data/videos-datasets/{input_file_name.avi} -i 0 -out_filename ./results/{output_file_name.avi}
\end{lstlisting}

Se ha descargado los pesos de \gls{yolov4} previamente entrenados sobre el dataset \gls{coco}. Por último se ha ejecutado el algoritmo de detección de objetos. En el código \ref{lst:evaluate-yolov4-darknet3} se especifica que la detección es en formato vídeo, se va a utilizar \gls{yolov4} como detector y la ubicación tanto del vídeo de entrada que se quiere procesar como el vídeo de salida resultante de las detecciones.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación del detector de objetos YOLOv4 en Darknet (3),captionpos=b,label={lst:evaluate-yolov4-darknet3}]
# Descargar los pesos preentrenados de YOLOv4
wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights

# Ejecutar detector de objetos en video con YOLOv4 en Darknet
./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights ./data/videos-datasets/{input_file_name.mp4} -i 0 -out_filename ./results/{output_file_name.avi}
\end{lstlisting}

En la figura \ref{fig:detection-yolov4-darknet-pets2007} se muestra el funcionamiento de \gls{yolov4} en Darknet donde se puede observar un escenario lleno de personas y equipajes como bolsas de mano, mochilas y maletas.

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{img/chapters/desarrollo/S08-fourthView-detection.jpg}
\caption{\label{fig:detection-yolov4-darknet-pets2007}Detección de objetos con YOLOv4 en Darknet}
\end{figure}

Las últimas evaluaciones del algoritmo de detección de \gls{yolov4} en Darknet se han realizado con Google Colab, un servicio cloud de Google basado en los Notebooks de Jupyter donde ya vienen instaladas la mayoría de librerías necesarias para la programación de algoritmos de Machine Learning. En el siguiente \href{https://colab.research.google.com/drive/1kXcZL7iZ2sZmyFuQ6GwVFfkfyha6A6nr?usp=sharing}{link} se puede ejecutar el Notebook de Google Colab donde se realizan los mismos pasos descritos anteriormente.

Durante el desarrollo del proyecto se ha decidido convertir el modelo de Darknet a Tensorflow, ya que se trata de un framework que emplea Python y facilitará la tarea de diseño del algoritmo de detección de objetos al no basarse únicamente en Darknet, framework que no es de uso extendido y puede ser una tarea complicada su utilización cuando aparezcan posibles errores.

Para la utilización de \gls{yolov4} basado en Tensorflow se necesita convertir el modelo. Para ello, se ha utilizado el repositorio de GitHub \cite{yolov4-tf-github}. A continuación se describen los pasos que se han seguido para poner en funcionamiento de \gls{yolov4} sobre Tensorflow en la versión 2.3.0rc0.

Primero se ha clonado el repositorio y se ha instalado las librerías y dependencias necesarias que se contemplan en el fichero \texttt{requirements-gpu.txt}. Para facilitar su instalación se ha creado con Anaconda un entorno virtual. La instalación de Anaconda y el entorno virtual se explica con mayor detalle en la sección \ref{subsec:instalacion-anaconda} y en la sección \ref{subsec:creacion-entorno}.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación del detector de objetos YOLOv4 en Tensorflow (1),captionpos=b,label={lst:evaluate-yolov4-tf1}]
# Descarga del repositorio de GitHub
git clone https://github.com/jmudy/tensorflow-yolov4-tflite

# Entrar dentro de la carpeta del repositorio
cd tensorflow-yolov4-tflite

# Descarga de los pesos de YOLOv4
wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights -P ./data/

# Crear entorno de Anaconda basado en Python 3.7.0 y acceder a el
conda env create -f conda-gpu.yml
conda activate yolov4-gpu
\end{lstlisting}

En el código \ref{lst:evaluate-yolov4-tf2} se muestra como convertir el modelo preentrenado de \gls{yolov4} en Darknet a Tensorflow. Se ha especificado un tamaño de 608x608, por dos motivos. El primero, es el es el mismo tamaño de redimensionamiento de las imágenes a la entrada de la capa de la red que en Darknet, con lo cual, se puede comparar ambos modelos en las mismas condiciones. Segundo, se han obtenido mejores resultados que empleando el clásico tamaño de 416x416.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación del detector de objetos YOLOv4 en Tensorflow (2),captionpos=b,label={lst:evaluate-yolov4-tf2}]
# Convertir pesos de YOLOv4 Darknet a Tensorflow
python save_model.py --weights ./data/yolov4.weights --output ./checkpoints/yolov4-608 --input_size 608 --model yolov4
\end{lstlisting}

En el código \ref{lst:evaluate-yolov4-tf3} se puede aprecia el comando se que ha utilizado para la ejecución del algoritmo de detección de objetos en \gls{yolov4} en Tensorflow. Como se puede observar en las primeras líneas del script \texttt{detect\_video.py} \cite{yolov4-tf-github}, se disponen de una serie de flags donde se puede indicar una gran variedad de parámetros. En este caso, se ha indicado que se quiere utilizar un modelo de \gls{yolov4} convertido con tamaño 608x608 (si no se indica el código por defecto busca un modelo de tamaño 416x416). También se ha especificado que se quiere emplear como detector \gls{yolov4}, ya que este repositorio también permite ejecutar la detección de objetos sobre \gls{yolo}v3. Del mismo modo que con Darknet, en el siguiente \href{https://colab.research.google.com/drive/1ZwcfV2hFZKcsyXaqKp9AGuEi5TY-QwVW?usp=sharing}{link} se puede consultar el Notebook de Google Colab donde se realizan los mismos pasos descritos anteriormente.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación del detector de objetos YOLOv4 en Tensorflow (3),captionpos=b,label={lst:evaluate-yolov4-tf3}]
# Ejecutar algoritmo de deteccion de objetos YOLOv4 en Tensorflow
!python detect_video.py --weights ./checkpoints/yolov4-608 --size 608 --model yolov4 --video {input_file_name.mp4} --output {output_file_name.avi}
\end{lstlisting}

En la figura \ref{fig:detection-yolov4-darknet-gba-far-video3} se muestra un ejemplo del funcionamiento de \gls{yolov4} en Tensorflow donde se puede observar el hall de una Universidad llena de personas y equipajes como bolsas de mano y maletas.

\begin{figure}[ht]
\centering
\includegraphics[width=0.45\textwidth]{img/chapters/desarrollo/GBA-far-video3-detection.jpg}
\caption{\label{fig:detection-yolov4-darknet-gba-far-video3}Detección de objetos con YOLOv4 en Tensorflow}
\end{figure}

Como se puede observar en la figura \ref{fig:predictions-darknet-tf} se puede apreciar que la precisión en las detecciones de \gls{yolov4} en Darknet y Tensorflow son prácticamente idénticas. Por tanto, se continuará el desarrollo del proyecto empleando Tensorflow como framework para la evaluación y diseño de los distintos algoritmos utilizados. En la sección \ref{subsec:resultados-yolov4-tf} se analizará con detalle las detecciones de objetos de \gls{yolov4} en Tensorflow.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{img/chapters/desarrollo/predictions.jpg}
    \caption{}
    \label{fig:predictions-darknet}
  \end{subfigure}
  \qquad\qquad
  \begin{subfigure}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{img/chapters/desarrollo/detection1.png}
    \caption{}
    \label{fig:predictions-tf}
  \end{subfigure}
  \caption{Detecciones de YOLOv4 con Darknet y Tensorflow.
    (\protect\subref{fig:predictions-darknet}) Detección de YOLOv4 con Darknet.
    (\protect\subref{fig:predictions-tf}) Detección de YOLOv4 con Tensorflow.}
  \label{fig:predictions-darknet-tf}
\end{figure}

\section{Datasets utilizados para el evaluación de YOLOv4}
\label{sec:datasets-utilizados}

La detección de objetos con \gls{yolov4} está basada en \gls{coco}, un dataset que contiene 80 categorías de objetos muy diversos y que se adaptan excelentemente a la problemática que se presenta en este proyecto. Contiene imágenes de las clases de interés: personas, mochilas, bolsos y maletas.

Para poder contrastar los resultados obtenidos en la evaluación de las métricas de calidad de \gls{coco} sobre \gls{yolov4}, se va a emplear el dataset \gls{oidv4}, donde previamente se entrenará un modelo de detección en base a las imágenes de \gls{oidv4}. La ventaja de utilizar \gls{oidv4} es que dispone de 600 categorías de objetos para evaluar los algoritmos de detección de objetos. De esta manera, se puede construir un gran dataset más específico y además con más clases de interés que las que ofrece \gls{coco}.

En las siguientes secciones se explicará en mayor profundidad y detalle como están formados los datasets \gls{coco} y \gls{oidv4}.

\subsection{MS COCO Dataset}
\label{subsec:coco-dataset}

El dataset \gls{coco} \cite{lin2015microsoft} es un conjunto de datos de referencia utilizado para evaluar el rendimiento de los modelos entrenados por visión por computadora. Está diseñado para representar una amplia gama de objetos que encontramos regularmente en la vida cotidiana (ver figura \ref{fig:cocodataset}).

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{img/chapters/desarrollo/cocodataset.png}
\caption{\label{fig:cocodataset}Categorías de objetos del dataset MS COCO \cite{coco-official-website}}
\end{figure}

\gls{coco} está etiquetado en un formato especial llamado COCO JSON, y proporciona datos para entrenar modelos supervisados de visión por computadora que son capaces de identificar los objetos comunes del conjunto de datos. Estos modelos están lejos de ser perfectos, por lo que el dataset \gls{coco} proporciona un punto de referencia para evaluar la mejora periódica de estos modelos a través de la investigación en visión por computadora.

Otra motivación para el dataset \gls{coco} es proporcionar un conjunto de datos base para entrenar modelos de visión por computadora. Una vez entrenado el modelo, se puede perfeccionar para aprender otras tareas, como las que se van a describir a continuación.

\subsubsection*{Tareas de MS COCO}
\label{subsubsec:tareas-coco}

\gls{coco} tiene múltiples tareas de visión por computadora. A continuación, se enumeran en orden decreciente en base a su uso:

\begin{itemize}
    \item \textbf{Detección de objetos}: los objetos se anotan con un cuadro delimitador y una etiqueta de clase. El dataset \gls{coco} tiene 121.408 imágenes para detección de objetos, 883.331 anotaciones de objetos, 80 clases y una resolución media de imagen de 640x480.
    
    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth]{img/chapters/desarrollo/detection-splash.png}
    \caption{\label{fig:detection-splash}MS COCO detección de objetos \cite{coco-official-website}}
    \end{figure}
    
    \item \textbf{Segmentación semántica}: los límites de los objetos se etiquetan con una máscara y las clases de objetos se etiquetan con una etiqueta de clase. La segmentación semántica requiere modelos para trazar los límites entre los objetos.
    
    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{img/chapters/desarrollo/panoptic-splash.png}
    \caption{\label{fig:panoptic-splash}MS COCO segmentación semántica \cite{coco-official-website}}
    \end{figure}    
    
    \item \textbf{Detección de puntos clave}: las personas son etiquetadas con puntos claves de interés (como pueden ser codos, rodillas o cabezas). El dataset \gls{coco} dispone de 250.000 personas con puntos clave etiquetados.
    
    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{img/chapters/desarrollo/keypoints-splash-big.png}
    \caption{\label{fig:keypoints-splash-big}MS COCO detección de puntos clave \cite{coco-official-website}}
    \end{figure}    
    
\end{itemize}

\subsection{Open Images Dataset v4}
\label{subsec:OIDv4-dataset}

\gls{oidv4} \cite{Kuznetsova_2020} es un conjunto de datos de 9,2 millones de imágenes con anotaciones en formato .txt unificadas para la clasificación de imágenes, detección de objetos y detección de relaciones visuales (ver figura \ref{fig:example-annotations-oidv4}). Las imágenes tienen una licencia Creative Commons Attribution que permite compartir y adaptar el material descargado de Flickr sin una lista predefinida de nombres de clases o etiquetas.

\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{img/chapters/desarrollo/example-annotations-oidv4.jpg}
\caption{\label{fig:example-annotations-oidv4}Ejemplo anotaciones en Open Images Dataset v4 \cite{Kuznetsova_2020}}
\end{figure}

\gls{oidv4} ofrece una gran escala en varias dimensiones: 30,1 millones de etiquetas a nivel de imagen para 19,8 mil conceptos, 15,4 millones de cuadros delimitadores para las 600 clases de objetos que se muestran en la figura \ref{fig:oidv4dataset}, y 375 mil anotaciones de relaciones visuales que involucra a 57 clases. Para la detección de objetos se proporciona más de 15 veces cuadros delimitadores que otros grandes datasets como \gls{coco} o ImageNet. Las imágenes suelen mostrar escenas complejas con varios objetos (de promedio tiene 8 objetos anotados por imagen). 

\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{img/chapters/desarrollo/oidv4-classes.png}
\caption{\label{fig:oidv4dataset}Categorías de objetos del dataset Open Images Dataset v4 \cite{Kuznetsova_2020}}
\end{figure}

\section{Evaluación de las métricas de calidad de MS COCO dataset}
\label{sec:evaluate-cocodataset}

En está sección se va a realizar la evaluación de las métricas de calidad del dataset \gls{coco} para tener una referencia a la hora de valorar, si reentrenando la red, se pueden obtener mejores métricas para la aplicación del proyecto.

Primero se ha descargado el repositorio oficial de \gls{yolov4} de GitHub \cite{yolov4-darknet-github}. Para aprovechar maximizar la eficiencia de la evaluación se ha modificado el fichero Makefile para habilitar la \gls{gpu} junto con \gls{cuda} y OpenCV. En esta evaluación se ha utilizado una NVIDIA Tesla T4, por lo que también ha sido necesario especificar que se va a utilizar una \gls{gpu} con una capacidad de cálculo de 7,5.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación de las métricas de calidad de MS COCO en YOLOv4 Darknet (1),captionpos=b,label={lst:evaluate-cocodataset1}]
# Clonar el repositorio de GitHub de YOLOv4 darknet
git clone https://github.com/AlexeyAB/darknet

# Entrar dentro de la carpeta del repositorio
cd darknet

# Modificar las siguientes lineas del fichero makefile para habilitar la GPU y OPENCV
sed -i 's/OPENCV=0/OPENCV=1/' Makefile
sed -i 's/GPU=0/GPU=1/' Makefile
sed -i 's/CUDNN=0/CUDNN=1/' Makefile
sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile
sed -i 's/# ARCH= -gencode arch=compute_75/ARCH= -gencode arch=compute_75/' Makefile

# Compilar mediante el comando make
make
\end{lstlisting}

Dentro de la carpeta del repositorio se ha creado una carpeta llamada \texttt{images} donde se almacenarán las imágenes del entrenamiento y validación del modelo de \gls{yolov4}. Tal y como se muestra en el código \ref{lst:evaluate-cocodataset2} se han descargado los ficheros comprimidos de las imágenes desde la web oficial de \gls{coco} \cite{coco-official-website}.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación de las métricas de calidad de MS COCO en YOLOv4 Darknet (2),captionpos=b,label={lst:evaluate-cocodataset2}]
# Crear y entrar en la carpeta ./images donde descargaremos las imagenes
mkdir images
cd images

# Descargar, descomprimir y borrar archivo comprimido .zip de las imagenes de entrenamiento
wget -c http://images.cocodataset.org/zips/train2017.zip
unzip -q train2017.zip
rm train2017.zip

# Descargar, descomprimir y borrar archivo comprimido .zip de las imagenes de validacion
wget -c http://images.cocodataset.org/zips/val2017.zip
unzip -q val2017.zip
rm val2017.zip

# Vuelta a la carpeta raiz
cd ..
\end{lstlisting}

En el siguiente \href{https://drive.google.com/drive/folders/1KLf2PTMwPmopPxPdFLtx-nAPpdlcT-NF?usp=sharing}{link} se encuentran los ficheros \texttt{train2017.txt} y \texttt{val2017.txt}, que contienen las rutas relativas de las imágenes de entrenamiento y validación. Se han copiado dentro de la carpeta \texttt{./data}. Por otro lado, se han copiado los ficheros de las etiquetas de las imágenes de entrenamiento que se encuentran dentro de la carpeta comprimida \texttt{./labels/train2017} y pegado dentro de la carpeta \texttt{./images/train2017} del repositorio junto a las imágenes de entrenamiento. Del mismo modo, se han copiado los ficheros de las etiquetas de las imágenes de validación que se encuentran dentro de la carpeta comprimida \texttt{./labels/val2017} y pegado dentro de la carpeta \texttt{./images/val2017} del repositorio junto a las imágenes de validación.

Es necesario modificar el fichero \texttt{./cfg/coco.data} para especificar por un lado, el número de clases que tiene el modelo que se quiere evaluar (no se ha tocado ya porque viene por defecto las 80 clases que componen el dataset \gls{coco}), la ubicación de los ficheros \texttt{train2017.txt} y \texttt{val2017.txt} y el fichero \texttt{coco.names} que contiene el nombre de cada una de las 80 clases.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Fichero coco.data,captionpos=b,label={lst:coco-data-file}]
classes = 80
train  = data/train2017.txt
valid  = data/val2017.txt
names = data/coco.names
#backup = /home/pjreddie/backup/
\end{lstlisting}

Por último, se han descargado los pesos de \gls{yolov4} preentrado y ejecutado el siguiente comando para evaluar las métricas de calidad en base al dataset \gls{coco}.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación de las métricas de calidad de MS COCO en YOLOv4 Darknet (3),captionpos=b,label={lst:evaluate-cocodataset3}]
# Descargar los pesos preentrenados de YOLOv4
wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights

# Ejecutar evaluacion de las metricas de calidad de MS COCO sobre YOLOv4
./darknet detector map ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights
\end{lstlisting}

Al finalizar el proceso de evaluación de las métricas se observan las métricas de calidad más relevantes. Los resultados obtenidos servirán de referencia a la hora de comparar con los resultados que se obtengan en el entrenamiento de \gls{yolov4} con el dataset \gls{oidv4}. En la sección \ref{subsec:metricas-calidad} se explicarán con mayor detalle cada una de las métricas que se han obtenido en la evaluación y en la sección \ref{subsec:metricas-calidad-coco}, se expondrán los resultados obtenidos tras la evaluación de las métricas de calidad.

Se ha creado un Notebook de Google Colab para la evaluación de las métricas de calidad de \gls{coco} donde se ha seguido el mismo procedimiento de esta sección. Se puede acceder entrando en este \href{https://colab.research.google.com/drive/19P6tIicaeD9vOTCl42eI9PR1nXVlsfoH?usp=sharing}{link}.

\section{Entrenamiento YOLOv4 con Open Image Dataset v4}
\label{sec:train-openimagesv4}

En las siguientes secciones se describen los pasos que se han seguido para entrenar un modelo de \gls{yolov4} con Darknet a partir de un dataset personalizado basado en imágenes de \gls{oidv4}.

\subsection{Recopilación y etiquetado del dataset personalizado}
\label{subsec:recopilacion-etiquetado-custom-dataset}

Para descargar las imágenes de entrenamiento y validación que se utilizarán en el entrenamiento del modelo \gls{yolov4} se ha utilizado la herramienta \gls{oidv4}\_Toolkit \cite{OIDv4_ToolKit}. Se trata de un repositorio disponible en GitHub que permite la descarga de imágenes del dataset de \gls{oidv4} para el entrenamiento de algoritmos de detección. Se ha decidido utilizar está herramienta porque tiene disponible un script que convierte las etiquetas del formato por defecto de \gls{oidv4} a Darknet.

Primero se ha descargado el repositorio de GitHub \cite{OIDv4_ToolKit} e instalado las librerías y dependencias necesarias. Se ha instalado un entorno virtual con Anaconda para que no interfiera con las versiones de las librerías con los otros repositorios utilizados en el proyecto.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Descarga dataset Open Images Dataset v4 (1),captionpos=b,label={lst:download-oidv4_1}]
# Clonar el repositorio de Github
git clone https://github.com/theAIGuysCode/OIDv4_ToolKit.git
cd OIDv4_ToolKit

# Instalacion de las librerias y dependendencias (recomendable crear un entorno virtual con Anaconda)
pip install -r requirements.txt
\end{lstlisting}

Ejecutando los comandos del código \ref{lst:download-oidv4_2} se han descargado las imágenes de entrenamiento y validación de las clases: \texttt{`Person', `Handbag', `Backpack'} y \texttt{`Suitcase'}. Se ha limitado a 1.500 las imágenes de cada clase para el entrenamiento y a 300 las imágenes de validación (20\% del tamaño del entrenamiento).

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Descarga dataset Open Images Dataset v4 (2),captionpos=b,label={lst:download-oidv4_2}]
# Descarga de las imagenes de entrenamiento con un limite de 1500
python main.py downloader --classes Person Handbag Backpack Suitcase --type_csv train --limit 1500 --multiclasses 1

# Descarga de las imagenes de validacion con un limite de 300
python main.py downloader --classes Person Handbag Backpack Suitcase --type_csv validation --limit 300 --multiclasses 1
\end{lstlisting}

La estructura de las etiquetas del dataset de Open Images Dataset v4 sigue la siguiente distribución de atributos: \{\texttt{nombre\_de\_la\_clase, left\_x, top\_y, right\_x, bottom\_y}\} (ver figura \ref{fig:bbox-oidv4}).

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{img/chapters/resultados/datasets/bbox-oidv4.png}
\caption{\label{fig:bbox-oidv4}Estructura de las etiquetas de Open Images Dataset v4 \cite{OIDv4_ToolKit}}
\end{figure}

Las etiquetas que se obtienen mediante esta herramienta no tienen el formato deseado. Las etiquetas en Darknet tienen la siguiente estructura \{\texttt{label\_ID, x\_center\_norm, y\_center\_norm, width\_norm, height\_norm}\}

donde:

\begin{itemize}
    \item \textbf{label\_ID}: es un número de índice del fichero \texttt{coco.names}, empezando por el 0 y seguido de números enteros crecientes.
    \item \textbf{x\_center\_norm} = x\_center\_abs/image\_width
    \item \textbf{y\_center\_norm} = y\_center\_abs/image\_height
    \item \textbf{x\_center\_norm} = width\_of\_the\_label\_abs/image\_width
    \item \textbf{x\_center\_norm} = height\_of\_the\_label\_abs/image\_height
\end{itemize}

Es preciso señalar que todos los atributos de posición de las etiquetas de Darknet no son absolutos, sino normalizados.

Afortunadamente, el repositorio \cite{OIDv4_ToolKit} dispone del script \texttt{convert\_annotations.py} para convertir el etiquetado tanto de las imágenes de entrenamiento como las de validación del dataset \gls{oidv4} a Darknet. Antes de su ejecución, se han sustituido en el fichero \texttt{classes.txt}, situado en la raíz del repositorio, las clases que habían por defecto por el nombre de las clases que se han descargado las imágenes, en el mismo orden que cuando se nombraron en el código \ref{lst:download-oidv4_2} para que la conversión se realice correctamente. Después se puede ejecutar la siguiente línea de código:

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Descarga dataset Open Images Dataset v4 (3),captionpos=b,label={lst:download-oidv4_3}]
# Convertir etiquetas al formato de Darknet
python convert_annotations.py
\end{lstlisting}

Al finalizar la conversión de las etiquetas, se han borrado las carpetas \texttt{Label} que contienen el etiquetado en el formato original. Las carpetas \texttt{obj} y \texttt{test} se han copiado a la carpeta \texttt{./data} del repositorio de \gls{yolov4} con Darknet, ya que se realizará el entrenamiento del modelo \gls{yolov4} en base al dataset personalizado en ese repositorio \cite{yolov4-darknet-github}.

\subsection{Configuración de ficheros para el entrenamiento}
\label{subsec:configuracion-ficheros-training}

Antes de comenzar el entrenamiento de la red con el dataset \gls{oidv4} es necesario configurar el fichero \texttt{.cfg} donde se encuentran las variables de la \gls{cnn} de \gls{yolov4}.

Se ha generado el fichero \texttt{yolov4-obj.cfg} basado en \texttt{yolov4.cfg}, y se han modificado los siguientes parámetros en base a las recomendaciones indicadas en \cite{yolov4-darknet-github} para el entrenamiento:

\begin{itemize}
    \item \texttt{batch} = \textbf{64} (Cantidad de imágenes y etiquetas que se calculan de una pasada en el cálculo del gradiente y se actualiza los pesos a través del \textit{backpropagation})
    \item \texttt{subdivisions} = \textbf{16} (Cantidad de \texttt{batches} que se subdividen en cada bloque. Las imágenes de cada bloque se ejecutan en paralelo en la \gls{gpu})
    \item \texttt{width} = \textbf{416} (Ancho del tamaño de la red. Cada imagen se redimensiona al tamaño de la red durante el entrenamiento y la detección)
    \item \texttt{height} = \textbf{416} (Altura del tamaño de la red. Cada imagen se redimensiona al tamaño de la red durante el entrenamiento y la detección)
    \item \texttt{max\_batches} = (\# de clases) * 2.000 = 4 * 2.000 = \textbf{8.000} (Número máximo de \texttt{batches})
    \item \texttt{steps} = \textbf{6.400} (80\% de max\_batches), \textbf{7.200} (90\% de max\_batches) (Ajuste del \texttt{learning rate} después de un determinado número de \texttt{batches})
    \item \texttt{filters} = (\# de clases + 5) * 3 = (4 + 5) * 3 = \textbf{27} (Número de núcleos convolucionales que contiene cada capa)
\end{itemize}

Se ha creado el fichero \texttt{obj.data} basado en el \texttt{coco.data} que se vio en la sección \ref{sec:evaluate-cocodataset}. En este caso, se ha especificado que el número de clases que se va a entrenar es de 4, la ubicación de los ficheros \texttt{train.txt}, \texttt{train.txt} y \texttt{coco.names} y la carpeta donde se generarán los ficheros \texttt{.weights} con los pesos de la red entrenada cada 1.000 iteraciones.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Fichero obj.data,captionpos=b,label={lst:obj-data-file}]
classes = 4
train = data/train.txt
valid = data/test.txt
names = data/obj.names
backup = /mydrive/yolov4/backup
\end{lstlisting}

Otro fichero generado ha sido el \texttt{obj.names}, basado en el \texttt{coco.names} que trae por defecto \gls{yolov4}. En este fichero se han sustituido las 80 clases de \gls{coco} que vienen predeterminadas por las 4 clases que se desean entrenar. El orden de escritura ha sido el mismo que el fichero \texttt{classes.txt}. Como ya se explicó en la sección \ref{subsec:recopilacion-etiquetado-custom-dataset}, el primer parámetro de las etiquetas de las imágenes de entrenamiento y validación es el ID de cada clase, por lo que debe de corresponder a la misma que se escribe en este fichero.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Fichero obj.names,captionpos=b,label={lst:obj-names-file}]
Person
Handbag
Backpack
Suitcase
\end{lstlisting}

Los últimos archivos de configuración necesarios antes de comenzar el entrenamiento son los ficheros \texttt{train.txt} y \texttt{train.txt}, los cuales contienen las rutas relativas a todas las imágenes de entrenamiento e imágenes de validación.

Ejecutando el siguiente script en Python se genera el fichero \texttt{train.txt}.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Generación del fichero train.txt,captionpos=b,label={lst:train-generate}]
import os

image_files = []
os.chdir(os.path.join("data", "obj"))
for filename in os.listdir(os.getcwd()):
    if filename.endswith(".jpg"):
        image_files.append("data/obj/" + filename)
os.chdir("..")
with open("train.txt", "w") as outfile:
    for image in image_files:
        outfile.write(image)
        outfile.write("\n")
    outfile.close()
os.chdir("..")
\end{lstlisting}

Para generar el fichero \texttt{test.txt}:

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Generación del fichero test.txt,captionpos=b,label={lst:test-generate}]
import os

image_files = []
os.chdir(os.path.join("data", "test"))
for filename in os.listdir(os.getcwd()):
    if filename.endswith(".jpg"):
        image_files.append("data/test/" + filename)
os.chdir("..")
with open("test.txt", "w") as outfile:
    for image in image_files:
        outfile.write(image)
        outfile.write("\n")
    outfile.close()
os.chdir("..")
\end{lstlisting}

\subsection{Entrenamiento del dataset personalizado}
\label{subsec:training-custom-dataset}

Se ha descargado los pesos de las capas convolucionales del modelo preentrenado \texttt{yolov4.conv.137}. El uso de estos peso hace que el detector de objetos personalizado ayuda a que converja, a que sea mucho más preciso y que no tenga que entrenar tanto tiempo.

Como último paso, se ha ejecutado el comando para realizar el entrenamiento del detector de objetos personalizado con \gls{yolov4}. Dado que se ha utilizado un conjunto de imágenes para la validación, se ha utilizado el flag \texttt{-map} para que se genere un diagrama con el \gls{map} con la finalidad de poder visualizar la precisión del modelo entrenado.

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Entrenamiento del dataset personalizado,captionpos=b,label={lst:training-custom-dataset}]
# Descarga de los pesos de las capas convolucionales de YOLOv4
wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137

# Comenzar a entrenar la red neuronal del dataset personalizado de OIDv4
./darknet detector train data/obj.data cfg/yolov4-obj.cfg yolov4.conv.137 -dont_show -map
\end{lstlisting}

Durante el entrenamiento se puede visualizar por terminal información muy relevante como la que se muestra en la figura \ref{fig:metrics-during-train}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{img/chapters/resultados/metricas/metrics-during-training.png}
\caption{\label{fig:metrics-during-train}Métricas durante el entrenamiento de la red neuronal con el dataset de OIDv4}
\end{figure}

Cada cierto número de iteraciones se calcula el \gls{map} y otras métricas de interés. Siguiendo las recomendaciones de \cite{yolov4-darknet-github}, las iteraciones mínimas por cada clase para realizar un entrenamiento es de 2.000. En el primer entrenamiento del dataset personalizado se han realizado 8.000 iteraciones para 4 clases. En el caso de que se hubiera fijado unas máxima iteraciones superiores a las mínimas recomendadas, un indicio para saber que se debe finalizar el entrenamiento es cuando la media de las pérdidas (\textit{average loss}) no disminuye en las siguientes iteraciones. También, se debe de tener en cuenta que, en datasets con una dificultad alta, como es este caso, la media de pérdidas final suele ser de $\text{avg loss = 3.0}$.

Al finalizar el entrenamiento se han evaluado las métricas de calidad. Gracias al framework Darknet \cite{darknet13} es fácil poder evaluarlas aplicando el siguiente comando en el terminal:

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación métricas de calidad del dataset utilizado para el entrenamiento de la red neuronal de detección de objetos,captionpos=b,label={lst:darknet-map}]
# Evaluacion de metricas de interes
./darknet detector train data/obj.data cfg/yolov4-obj.cfg /mydrive/yolov4/backup/yolov4-obj_last.weights -dont_show
\end{lstlisting}

Durante el entrenamiento se ha generado un diagrama que se encuentra en la carpeta raíz del repositorio con nombre \texttt{chart.png}, donde se puede observar la evolución de las pérdidas por el error en función de las iteraciones. En la figura \ref{fig:chart-train} se visualiza en color azul la evolución de las pérdidas y en rojo la evolución del \gls{map}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{img/chapters/resultados/metricas/chart_train.png}
\caption{\label{fig:chart-train}Evolución del mAP y pérdidas a lo largo de las interacciones durante el entrenamiento de la red neuronal con el dataset de OIDv4}
\end{figure}

Cabe resaltar que, cada 1.000 iteraciones se genera un fichero \texttt{.weights} con los pesos del modelo que estamos entrenando en \gls{yolov4} y se almacenan en la carpeta \texttt{./backup}. Es importante tener en cuenta que se ha establecido las máximas iteraciones del entrenamiento a 8.000, sin embargo, es posible que el mejores resultados en las métricas hayan sido en la iteración 5.000. Esto puede ser producido por el \textit{overfitting}. Afortunadamente, durante el entrenamiento se genera un fichero llamado \texttt{yolov4-obj\_best.weights} donde se almacenan los pesos de la red entrenada con mejores resultados. Por tanto, se ha podido ahorrar tiempo en analizar las métricas cada 1.000 iteraciones para determinar en que iteración el error era mínimo.

En la sección \ref{subsec:metricas-calidad-openimagesv4} se observarán y analizarán los resultados tanto de este entrenamiento como de un segundo que ha sido necesario ya que no se obtuvieron métricas de calidad superiores a las del dataset \gls{coco}.

\section{Seguimiento de personas y objetos con YOLOv4 y Deep SORT}
\label{sec:desarrollo-yolov4+deepsort}

\textcolor{red}{Se utilizará el mismo entorno virtual que el de detección de objetos con YOLOv4 en Tensorflow}

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación del seguimiento de objetos Deep SORT y YOLOv4 en Tensorflow (1),captionpos=b,label={lst:evaluate-deepsort-tf1}]
# Descarga del repositorio de GitHub
git clone https://github.com/jmudy/yolov4-deepsort

# Entrar dentro de la carpeta del repositorio
cd yolov4-deepsort

# Cambiar a la rama de git develop y comprobar que nos encontramos en ella
git checkout develop
git branch -a

# Descarga de los pesos de YOLOv4
wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights -P ./data/
\end{lstlisting}

\textcolor{red}{bla bla bla}

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación del seguimiento de objetos Deep SORT y YOLOv4 en Tensorflow (2),captionpos=b,label={lst:evaluate-deepsort-tf2}]
# Ejecutar algoritmo de seguimiento de objetos con Deep SORT y YOLOv4 en Tensorflow
!python object_tracker.py --video {input_file_name.mp4} --output {output_file_name.avi} --model yolov4 --size 608 --weights ./checkpoints/yolov4-608 --count --info
\end{lstlisting}


\newpage

\section{Algoritmo de detección de objetos abandonados}
\label{sec:algoritmo-object-detection}

\textcolor{red}{Dibujar el esquema que voy a seguir para determinar cuando un objeto ha sido abandonado.}
\url{https://app.diagrams.net/}

\textcolor{red}{Intentar meter toda la chicha posible. Que quede pendiente meter aquí código de lo que he implementado para que no quede todo en el aire y se vean de repente los resultados, por lo menos que se vea la función que hace la asociación de persona objeto y la función que establece si la persona ha abandonado un objeto o si el objeto esta abandonado y sin propietario.}

\textcolor{red}{Poner figura ejemplo de la hipotesis donde se hace asociacion de persona y objeto cuando se activa la alarma y cuando se determina que un objeto ha sido abandonado, en el momento de la alarma se ejecuta una cuenta atrás de 30 segundos antes de determinar que el objeto ha sido abandonado.}

\textcolor{red}{Explicar la hipótesis de cuando un objeto no tiene propietario porque se encuentra alejado de cualquier persona en los primeros fotogramas del vídeo. En este caso al pasar 15 segundos se determina que el objeto ha sido abandonado.}

Cuando se calcula se calcula la distancia entre personas y objetos de interés se calcula la media de las distancias obtenidas fotograma a fotograma en los primeros 5 segundos de vídeo. Se crea una asociación de una persona al objeto que se haya obtenido la menor distancia en el momento de la asociación, siempre y cuando esa distancia sea inferior a dos veces el ancho del cuadro limitador del objeto, para así no depender de la profundidad a la que se pueda encontrar las personas y objetos dentro del plano de visión.

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{img/chapters/desarrollo/abandoned-object-scheme.png}
\caption{\label{fig:abandoned-object-scheme}Esquema hipótesis detección objeto abandonado}
\end{figure}

\newpage

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{img/chapters/desarrollo/link-persona-objeto.jpg}
\caption{\label{fig:link-persona-objeto}Asociación persona-objeto}
\end{figure}

Hola hola

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{img/chapters/desarrollo/warning-abandono.jpg}
\caption{\label{fig:warning-abandono}Aviso de alerta posible objeto abandonado}
\end{figure}

Hola hola

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{img/chapters/desarrollo/abandono-objeto-avss.jpg}
\caption{\label{fig:abandono-objeto-avss}Detección de objeto abandonado}
\end{figure}

Hola hola

\newpage

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación de la detección de objetos abandonados con DeepSORT y YOLOv4 en Tensorflow (1),captionpos=b,label={lst:evaluate-abandoned-object1}]
# Descarga del repositorio de GitHub
git clone https://github.com/jmudy/yolov4-deepsort

# Entrar dentro de la carpeta del repositorio
cd yolov4-deepsort

# Cambiar a la rama de git abandoned y comprobar que nos encontramos en ella
git checkout abandoned
git branch -a

# Descarga de los pesos de YOLOv4
wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights -P ./data/
\end{lstlisting}

\textcolor{red}{bla bla bla}

\vspace{0.5cm}
\begin{lstlisting}[language=iPython,caption=Evaluación de la detección de objetos abandonados con DeepSORT y YOLOv4 en Tensorflow (2),captionpos=b,label={lst:evaluate-abandoned-object2}]
# Ejecutar algoritmo de deteccion objetos abandonados con Deep SORT y YOLOv4 en Tensorflow
!python abandoned_object.py --video {input_file_name.mp4} --output {output_file_name.avi} --model yolov4 --size 608 --weights ./checkpoints/yolov4-608 --count --info
\end{lstlisting}


\newpage

\section{Conclusiones}
\label{sec:conclu-desarrollo}

\textcolor{red}{Hacer unas breves conclusiones de lo que se ha conseguido en base a los objetivos que se han marcado en la introducción de este capítulo \ldots}

\begin{algorithm}[H]
 \caption{How to write algorithms}
 \label{alg:howto}
 \KwData{this text}
 \KwResult{how to write algorithm with \LaTeX}
 initialization\;
 \While{not at end of this document}{
  read current\;
  \eIf{understand}{
   go to next section\;
   current section becomes this one\;
   }{
   go back to the beginning of current section\;
  }
 }
\end{algorithm}
